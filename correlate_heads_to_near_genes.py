# description: Calculates expression (read count) correlation between heads and its nearest gene(s). When only "relation" is said, I mean head -> nearest gene(s) mapping.
# in: pardir/'genome_annotation/head_genes_relations.tsv'
# in: pardir/'counted_reads'
# in: pardir/'genome_annotation/gene_annotations.gff3'
# in: pardir/'genome_annotation/head_annotations.gff3'
# out: pardir/'genome_annotation/head_genes_correlations.tsv'
# out: pardir/f'genome_annotation/head_genes_complements_correlations.tsv'
# out: pardir/'counted_reads/aggregated.tsv'

from utils import (pardir, show_flag, safe_open, parse_gff_attributes,
                   prinf, GFF3_COLUMNS, save_all_figs)
import pandas as pd
from matplotlib import pyplot as plt
from sys import argv

out_aggregated_counts = pardir/f'counted_reads/aggregated.tsv'

outpath = pardir/f'genome_annotation/head_genes_correlations.tsv'
complements_outpath = pardir/f'genome_annotation/head_genes_complements_correlations.tsv'

def main():
    outfile = safe_open(outpath, exist_ok='exit')
    complements_outfile = safe_open(complements_outpath, exist_ok='exit')

    print('Buscando comprimentos de genes e heads...')
    gene_attibutes = pd.read_table(pardir/'genome_annotation/gene_annotations.gff3',
                                   names=GFF3_COLUMNS, usecols=['attributes'])['attributes']
    head_attibutes = pd.read_table(pardir/'genome_annotation/head_annotations.gff3',
                                   names=GFF3_COLUMNS, usecols=['attributes'])['attributes']

    gene_lengths = parse_gff_attributes(gene_attibutes, gene_id='Name')['length']
    head_lengths = parse_gff_attributes(head_attibutes)['length']
    lengths = pd.concat([head_lengths, gene_lengths]).astype(int)

    # Consider same lengths for complements
    new_lengths = lengths.copy()
    new_lengths.index = lengths.index + '_complement'
    lengths = lengths.append(new_lengths)

    print('Concluído. Lendo arquivo de relações...')
    relations = pd.read_table(pardir/f'genome_annotation/head_genes_relations.tsv')

    # Escrever cabeçalho
    outfile.write('\t'.join(relations.columns) + '\tcorrelation\n')
    complements_outfile.write('\t'.join(relations.columns) + '\tcorrelation\n')


    # ###################### AGREGATE COUNTS ###############################
    print('Conclúido. Agregando contagens em um DataFrame...')

    # Read counts for each feature
    counts = pd.DataFrame()

    # for each SRA library
    for count_path in (pardir/'counted_reads').glob('*.csv'):  # WARNING: A extensão será mudada para tsv no futuro.
        print(f'Processing {str(count_path)}')

        lib_name = count_path.stem.split('_')[0]

        count = pd.read_table(count_path, header=None,
                              names=['feature', lib_name],
                              index_col='feature', sep='\s+')


        # This block does not run if count is being generated by counter script
        if not count.empty:

            ### NORMALIZE FOR TOTAL LIBRARY SIZE (total_count in millions):
            # total_count = counts sum
            # + __no_feature
            # + __ambiguous
            # + __too_low_aQual
            # (without) __not_aligned
            # + __alignment_not_unique
            total_count = count.iloc[:-2].sum() + count.iloc[-1]
            count /= total_count/1e6

            ### NORMALIZE FOR FEATURE LENGTH (IN KBP):
            count[lib_name] /= lengths/1000  # This makes final flag rows (__no_feature etc.) become NaN.
            count.dropna(inplace=True)  # This eliminates final flags.

            # count final meaning (RPKM):
            # read count by library size (in million reads) by feature length (in bp)
            counts = counts.combine_first(count)


    counts = counts.T
    counts.index.name = 'biblioteca'
    print(counts)

    out_aggregated_file = safe_open(out_aggregated_counts)
    if out_aggregated_file is not None:
        counts.to_csv(out_aggregated_file, sep='\t')
        out_aggregated_file.close()

    print('Concluído. Calculando correlações...')


    gridx, gridy = 5, 8
    plt.rcParams['font.size'] = 4
    plt.rcParams['figure.figsize'] = (16, 9)
    i = 0

    for complement_flag in (False, True):
        for _, relation_row in relations.iterrows():

            hid = relation_row.head_id
            gid = relation_row.gene_id

            if complement_flag:
                hid += '_complement'
                gid += '_complement'

            if hid not in counts:
                print(f'WARNING: {hid} não presente nas contagens, só nas relações head-gene. Talvez a contagem deva ser refeita.')
                continue

            head_col = counts[hid]
            gene_col = counts[gid]

            n_non_zero = (head_col.astype(bool) & gene_col.astype(bool)).sum()
            # if number of significative points (head and gene counts != 0) is less than 4
            if n_non_zero < 4:
                prinf('\nCorrelação descartada:\n', counts[[gid, hid]])
                continue

            prinf('\nCorrelação plotada:\n', pd.concat([counts[[gid, hid]], head_col.astype(bool) & gene_col.astype(bool)], 1))

            corr = head_col.corr(gene_col)

            if show_flag:
                gridi = i % (gridx*gridy) + 1
                plt.subplot(gridx, gridy, gridi)
                plt.plot(head_col, gene_col, 'o', label=corr)
                plt.legend()
                plt.title('_'.join(list(relation_row.astype(str)) + [str(n_non_zero)]))

                if gridi == gridx*gridy:
                    plt.tight_layout()
                    save_all_figs()
                    plt.show()

                i += 1

            line = '\t'.join([*relation_row.astype(str), str(corr)])+'\n'
            (outfile, complements_outfile)[complement_flag].write(line)

    outfile.close()
    complements_outfile.close()

    print(f'Arquivos de correlações salvos:\n{str(outpath)}\n{str(complements_outpath)}')


if __name__ == '__main__':
    main()
